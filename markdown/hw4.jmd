---
title: Homework 4
author: Long Nguyen
---

## 1. Monotonicity of Loss and Regularizer

We consider an example of regularized empirical risk minimization as follows,

```math
\mathcal{L}(θ) + λr(θ),\: where\: \mathcal{L}(θ) = \frac{1}{n}∑_iℓ(θ^Tx^i, y^i)
```

We further consider two different regularization parameters, ``λ`` and
``\widetilde{λ}``, such that ``0 < λ ≤ \widetilde{λ}``. Let's also define ``θ``
and ``\widetilde{θ}`` to be the minimizing parameters of ``\mathcal{L}(θ) +
λr(θ)`` and ``\mathcal{L}(θ) + \widetilde{λ}r(θ)``, respectively.

#### (a) Monotonicity of Regularizer

An interesting result we observe is that as the regularization parameter gets
larger, the regularization error will not. We could leverage the fact that each
``θ`` achieves the lowest value of its corresponding regularized loss function.

$$
\begin{align*}
\mathcal{L}(θ) + λr(θ) &≤ \mathcal{L}(\widetilde{θ}) + λr(\widetilde{θ}) \\
\mathcal{L}(\widetilde{θ}) + \widetilde{λ}r(\widetilde{θ}) &≤
    \mathcal{L}(θ) + \widetilde{λ}r(θ) \\
⟹ \mathcal{L}(θ)+λr(θ)+\mathcal{L}(\widetilde{θ})+\widetilde{λ}r(\widetilde{θ})
    &≤ \mathcal{L}(\widetilde{θ}) + λr(\widetilde{θ}) +
    \mathcal{L}(θ) + \widetilde{λ}r(θ) \\
⟹ λr(θ)+\widetilde{λ}r(\widetilde{θ}) &≤ λr(\widetilde{θ})+\widetilde{λ}r(θ) \\
⟹ (\widetilde{λ} - λ)r(\widetilde{θ}) &≤ (\widetilde{λ} - λ)r(θ)
\end{align*}
$$

If ``\widetilde{λ} = λ``, then the regularization error is the same. But if
``\widetilde{λ} > λ``, then it follows from our inequality above that
``r(\widetilde{θ}) ≤ r(θ)``.

#### (b) Monotonicity of Loss

We observe further that increasing ``λ`` will not reduce our loss function
``\mathcal{L}``. This follows easily from our previous result:

$$
\begin{align*}
\mathcal{L}(θ) + λr(θ) &≤ \mathcal{L}(\widetilde{θ}) + λr(\widetilde{θ}) \\
\mathcal{L}(\widetilde{θ}) + λr(\widetilde{θ}) &≤
    \mathcal{L}(\widetilde{θ}) + λr(θ) \\
⟹ \mathcal{L}(θ) + λr(θ) &≤ \mathcal{L}(\widetilde{θ}) + λr(θ) \\
⟹ \mathcal{L}(θ) &≤ \mathcal{L}(\widetilde{θ})
\end{align*}
$$

To sum up, with larger regularization parameter, our regularized error will
not increase, but our loss won't decrease either.

## 2. Common Loss Functions

In this section, we take a look at some popular choice of loss function: square
penalty, Huber penalty, and log Huber penalty.

1. Square penalty is relatively straight forward, ``p^{sqr}(r)=r^2``. Small
errors is not penalized as much, but significant difference from our predictions
result in large loss.
2. Huber penalty is quadratic for small errors, but linear for large ones. This
is so that our predictor is less sensitive to outliers.
3. Log Huber penalty follows the same rationale, but makes large errors even
smaller using the `log` function.

#### (a) Gradients

Below we implement the loss functions and their gradients:

```julia
### Abstract type for loss objectives
"""
    abstract type Loss

The abstract type for loss objectives. Concrete types subtyping `Loss` must
define a `penalty` method to specify the penalty function to calculate the loss,
and a `derivative` method to compute the derivative of such penalty function.
Loss is calculated as the mean of the penalty applied to the predictions and
observations.
"""
abstract type Loss end

# Useful error messages if required methods are not specified
penalty(l::Loss) = error("No `penalty` method defined for type $(typeof(l)).")
derivative(l::Loss) =
    error("No `derivative` method defined for type $(typeof(l)).")

# Loss objective evaluation
"""
    (l::Loss)(ŷ, y)

Compute the loss objective by taking the mean of the penalty applied to `ŷ - y`.
"""
(l::Loss)(ŷ, y) = sum(penalty(l), ŷ - y) / length(y)

# Loss objective gradient
"""
    gradient(l::Loss, ŷ, y)

Compute the gradient of the loss objective with the `derivative` method.
"""
gradient(l::Loss, ŷ, y) = derivative(l).(ŷ - y) / length(y)

### Square loss
"""
    struct SquareLoss

Self-explanatory loss function by squaring the error.
"""
struct SquareLoss <: Loss end

penalty(l::SquareLoss) = x -> x^2
derivative(l::SquareLoss) = x -> 2x

### Huber loss
"""
    struct HuberLoss

Loss objective using the Huber penalty function with parameter `α`.
"""
struct HuberLoss <: Loss
    α
end

penalty(l::HuberLoss) = x -> (abs(x) <= l.α ? x^2 : l.α * (2abs(x) - l.α))
derivative(l::HuberLoss) = x -> (abs(x) <= l.α ? 2 * x : 2 * l.α * x / abs(x))

### Log Huber loss

"""
    struct LogHuberLoss

Loss objective using the log Huber penalty function with parameter `α`.
"""
struct LogHuberLoss <: Loss
    α
end

penalty(l::LogHuberLoss) =
    x -> (abs(x) <= l.α ? x^2 : l.α^2 * (1 - 2log(l.α) + log(x^2)))
derivative(l::LogHuberLoss) = x -> (abs(x) <= l.α ? 2 * x : 2 * l.α^2 / x)
;
```

We leaverage Julia's multiple dispatch in the example code above. By defining
loss evaluation and gradient methods only for the abstract type `Loss`, we just
have to define the penalty and derivative for each of our specific losses, and
the interface of `Loss` will take care of the rest!

#### (b) Convexity

Convexity of a penalty loss function is a significant factor to consider, since
if the loss is convex, a numerical iterative method will be able to approximate
the optimal solution.

From the loss functions detailed in the previous section, the square and Huber
penalty losses are convex, since their second derivatives are nonnegative. The
log Huber loss, however, is not convex, since the second derivative is negative.

Therefore, it is not guaranteed that the log Huber loss will reach an optimal
solution, but the approach will be most resistant to outliers in our dataset.
In other words, the choice of the loss function is up to what we need our for
our predictor.

#### (c) Demonstrations

We demonstrate how different loss objectives affect training on our dataset
`loss.json`. The predictors to evaluate are:

1. ``ŷ = 15.5x + 5.9``
2. ``ŷ = 10x + 6.5``
3. ``ŷ = 5x + 7.9``

```julia
predict1(x) = 15.5x .+ 5.9
predict2(x) = 10x .+ 6.5
predict3(x) = 5x .+ 7.9
predictors = [predict1, predict2, predict3]
labels = ["y = 15.5x + 5.9" "y = 10x + 6.5" "y = 5x + 7.9"]
;
```

Let's try out each predictor with the loss functions defined so far.

```julia
using Plots

# Load dataset
include("./utils/readclassjson.jl")
data = readclassjson("./data/loss.json")
x = data["X"]
y = data["y"]

# Plotting
scatter(x, y, label="")
xrange = -3:0.1:3
plot!(xrange, [predict(xrange) for predict in predictors],
      labels=labels, legend=:right) |> display

# Compute losses
losses = [SquareLoss(), HuberLoss(1), LogHuberLoss(1)]
loss_labels = ["Square loss", "Huber loss (α = 1)", "Log Huber loss (α = 1)" ]
for (label, predict) in zip(labels, predictors)
    println("For predictor $label:")
    for (loss_label, loss) in zip(loss_labels, losses)
        println("\t$loss_label: ", loss(predict(x), y))
    end
end
```
From the graph and the losses, we observe empirical evidence that our intuition
about penalty functions are right: square loss is the lowest for the predictor
most affected by the outliers, while log Huber loss is very resistant to such
abnormalities.

## 3. Gradient Descent

#### (a) Julia Implementation

We explore the gradient descent method to optimize an objective function in this
section. Below we define a straightforward implementation of gradient descent:

```julia
using Statistics: norm

"""
    GD(X, y, loss[, max_iter=nothing, ϵ=1e-7, h=0.1])

Gradient descent on the input `X` and the output `y` using the loss function
provided. The algorithm stops when it reaches maximum iterations, or if the
Euclidean norm of the gradient is lower than or equal to `ϵ`.
"""
function GD(X, y, loss; max_iter=100, ϵ=1e-7, h=0.1)
    θ = randn(size(X, 2), size(y, 2))
    iter = 0

    while iter ≤ max_iter
        iter += 1

        # Compute loss gradient and break if gradient is small enough
        ∇ = X' * gradient(loss, X * θ, y)
        norm(∇) ≤ ϵ && break

        # Iterate to find better θ
        while true
            tent_θ = θ - h * ∇ # tentative update to θ
            if loss(X * tent_θ, y) ≤ loss(X * θ, y)
                θ = tent_θ
                h *= 1.2
                break
            else
                h *= 0.5
            end
        end
    end

    return θ
end
;
```

#### (b) Loss Optimization

We perform gradient descent on our dataset `gd.json` and find the optimal ``θ``
that minimizes the loss function.

```julia
# Load dataset
data = readclassjson("./data/gd.json")
X = data["X"]
y = data["y"]
num_data = size(X, 1)

scatter(X[:, 1], X[:, 2], y, label="", camera=(15, 30)) |> display
histogram(y, bins=50, label="", xlabel="Output", ylabel="Frequency")
```

From the plot, we observe that our data do not seem to have so many outliers. In
fact, the output is relatively normally distributed. We will therefore choose
the Huber loss to model our data, with the standard deviation as ``α``.

```julia; hold=true
using Statistics: std
include("./utils/regression.jl")

loss = HuberLoss(std(y))
affine_X = hcat(ones(size(X, 1)), X)

θ1 = GD(affine_X, y, loss)
θ2, = find_theta(affine_X, y)

@show θ1
@show θ2
;
```

We see that gradient descent approximate the optimal solution correctly to the
seventh decimal point (the tolerance error is ``10^{-7}``)!

#### (c) Gradient Descent Performace

While gradient descent is an important tool for optimization, the algorithm does
not necessarily converge for non-convex objective. Another important point to
keep in mind is that gradient descent could very well be stuck in a local minima
depending on the step size instead of the true optimal solution.
