---
title: Homework 2
author: Long Nguyen
---

## 1. Polynomial fit

In this section, we explore the polynomial fitting technique. The idea is to
extend linear regression by concatenating the input taken to higher powers
together to find a polynomial predictor.

#### (a) Simple Linear Regression

We take a look at the dataset in `polyfit.json`: let's try out linear regression
and evaluate our model with out-of-sample validation. We will split the dataset
into two, the train set and the test set. The model learns from the train set,
and we survey how it performs on the test set.

```julia; hold=true
include("./utils/readclassjson.jl") # simple function to parse json file
include("./utils/RMSE.jl") # our codes for RMSE from Homework 1
include("./utils/regression.jl") # codes for linear regression from Homework 1

using Random: randperm # simple way to randomize our dataset

"""
    split_dataset(x, y, ratio=0.8)

Given the dataset input `u`, output `v`, and the train-test ratio, return
randomized train and test sets.
"""
function split_dataset(x, y, ratio=0.8)
    @assert size(x, 1) == size(y, 1) "Number of inputs must equal outputs"

    # Randomize indices
    num_data = size(x, 1)
    rand_indices = randperm(num_data)

    # Split into train and test sets
    split_index = floor(Int, num_data * ratio)
    train_indices = rand_indices[1:split_index]
    test_indices = rand_indices[split_index+1:end]

    # Return datasets as named tuples
    train_set = (input=x[train_indices, :],
                 output=y[train_indices, :])
    test_set = (input=x[test_indices, :],
                output=y[test_indices, :])
    return train_set, test_set
end

# Load dataset and split into train and test set
data = readclassjson("./data/polyfit.json")
u, v = data["u"], data["v"]
train_set, test_set = split_dataset(u, v)
train_u, train_v = train_set.input, train_set.output
test_u, test_v = test_set.input, test_set.output

# Straightforward regression
θ, _, train_error = find_theta(train_u, train_v)
test_error = RMSE(test_u * θ, test_v)

@show train_error
@show test_error
;
```

The `split_dataset` function defined above gives us a convenient tool to split
dataset into train and test sets. The RMSE is not so encouraging, so let us
experiment with polynomial embeddings.

#### (b) Polynomial Embedding

Similar to how we transform a linear regression problem to an affine one by
embedding ``ϕ(u) = [1; u]``, computing regression with the embedding
``ϕ(u) = [1; u; u²;...;uᵈ⁻¹]`` gives us polynomial fitting between the input
and the output.

Let's define the embedding function:

```julia
"""
    polynomial_features(x, degree)

Return a polynomial embedding of degree `degree - 1` of the `n`-vector `x` and
return a `n x degree` matrix, where each column is the polynomial mapping of
each scalar in `x`.
"""
function polynomial_features(x, degree)
    embedded_x = Array{eltype(x)}(undef, length(x), degree)
    for d in 1:degree
        embedded_x[:, d] = x.^(d - 1) # Julia is column major
    end
    return embedded_x
end
;
```

One important remark is that `Julia` is column-major, so we should take care in
writing outer loops for higher array dimensions first instead of the other way
around in `Python`'s `numpy`.

#### (c) Polynomial Regression

Now let's jump right in polynomial embedding! We will test up to degree 20 to
survey which would perform the best. The true model for our dataset (with some
stochastic errors) is ``model(x) = 10sin(1.9 × 3.14 × x)``. We will train on
the entire dataset and compare it with the dataset itself as well as the model.

```julia
using Plots

model(x) = 10 * sin.(1.9 * 3.14 * x)

function test_polyfit(u, v, max_degree=20)
    dataset_RMSEs = Float64[]
    model_RMSEs = Float64[]

    # Evaluate polynomial fitting for each max degree
    for d in 1:max_degree
        embedded_u = polynomial_features(u, d)

        θ, _, dataset_error = find_theta(embedded_u, v)
        push!(dataset_RMSEs, dataset_error)

        model_error = RMSE(embedded_u * θ, model(u))
        push!(model_RMSEs, model_error)

        # Report RMSE
        println("Polynomial regression of degree $d:")
        println("\tRMSE for dataset: $dataset_error")
        println("\tRMSE for model: $model_error")
    end
    return dataset_RMSEs, model_RMSEs
end

dataset_RMSEs, model_RMSEs = test_polyfit(u, v)

plot(1:20, [dataset_RMSEs model_RMSEs],
     label=["Dataset RMSE" "Model RMSE"], xticks=1:20,
     xlabel="Polynomial Fitting Degree", ylabel="RMSE")
```

From the graph comparing the degree of the polynomial fitting and the resulting
root-mean-squared error, embedding our input with degree 4 appears to achieve
the best fit for our true model! Let's visualize different models on our
dataset: we will pick an underfitting one of degree 3, the best fit with
degree 4, and an overfitting model with degree 7.

```julia
xrange = 0:0.01:1
degrees = [3, 4, 7]
polyfit_labels = ["Underfitting", "Best fit", "Overfitting"]
polyfit_colors = [:yellow, :green, :red]

scatter(u, v, label="Dataset")
plot!(xrange, model(xrange), label="True model", c=:blue)
for i in 1:3
    d = degrees[i]
    embedded_u = polynomial_features(u, d)
    θ, _, _ = find_theta(embedded_u, v)
    y = polynomial_features(xrange, d) * θ

    plot!(xrange, y, label=polyfit_labels[i], c=polyfit_colors[i])
end
plot!(xlabel="u", ylabel="v")
```

#### (d) Cross Validation

In order to better gauge how well our polynomial regressions do on unseen data
sampled from the same population, cross validation is a useful technique. We
divide our dataset into k-folds, and then make each fold the test set while we
train on the rest. The average and the standard deviation of the RMSEs then give
us some metrics to expect how our model would perform.

```julia
using Statistics: mean, std
using Base.Iterators: partition

"""
    cross_validation(X, y, k)

Given the `n x d` matrix `X`, the label vector `y`, and the `k` number of folds,
return the average and standard deviation of the RMSE by cross validation.
"""
function cross_validation(X, y, k)
    @assert k > 1 "There must be more than one fold."

    # Randomize data
    num_data = size(X, 1)
    random_indices = randperm(num_data)

    # Divide into folds
    fold_size = floor(Int, num_data / k) # get fold size
    fold_indices = collect(partition(random_indices[1:k*fold_size], fold_size))
    # Append the rest, if there are any, to the last fold
    if num_data % k != 0
        append!(fold_indices[end], random_indices[k*fold_size+1:end])
    end

    RMSEs = Float64[]
    for fold in fold_indices
        # Set up train set and test set
        train_indices = trues(num_data)
        train_indices[fold] .= false
        test_X, test_y = X[fold, :], y[fold]
        train_X, train_y = X[train_indices, :], y[train_indices]

        # Regression
        _, _, error = find_theta(train_X, train_y)
        push!(RMSEs, error)
    end

    return mean(RMSEs), std(RMSEs, corrected=false)
end

function cross_validate_polyfit(u, v, k=5, max_degree=20)
    RMSEs = Float64[]
    RMSE_stds = Float64[]

    # Loop through each degree of polynomial embedding
    for d in 1:max_degree
        embedded_u = polynomial_features(u, d)
        RMSE, RMSE_std = cross_validation(embedded_u, v, k)
        push!(RMSEs, RMSE)
        push!(RMSE_stds, RMSE_std)
    end

    return RMSEs, RMSE_stds
end

RMSEs, RMSE_stds = cross_validate_polyfit(u, v)
plot(1:20, RMSEs, label="",
     xticks=1:20, yerror=RMSE_stds,
     xlabel="Polynomial embedding degree",
     ylabel="Cross-validated RMSE")
```

## 2. Introduction to Feature Engineering

We take a closer look at feature engineering in this section. It is clear good
model design matters, and so having a good knowledge of different embeddings and
when to use them will come in handy as we dig deeper into machine learning.

#### (a) Embedding Design

![](../images/HW2-2A.png)

1. In figure 2a, the decision boundary line could be an elliptical centered at
   the origin. Therefore, we could embed ``x = u^2`` and ``y = v^2``, then find
   an affine predictor between ``x`` and ``y``.
2. In figure 2b, a possible candidate for the solution is a logarithm curve. We
   should embed ``x = u`` and ``y = exp(v)`` to find an affine predictor.
3. Figure 3b hints at a polynomial curve, and so we could experiment with
   polynomial fitting for ``u`` to find the best fit.

#### (b) Categorical Embeddings

1. Type of transportation is nominal data, so one-hot is an obvious solution.
2. Service rating is ordinal, but does not provide accurate ratio information.
   An alternative solution is to embed the rank as categorical. For example, we
   provide four categories: at least poor, at least average, at least good, and
   at least excellent. An average will then be encoded as ``[1, 1, 0, 0]``.
3. Sex has two main options, so Boolean embedding comes across as an obvious
   solution. However, intersex people do exist, and we might want to make our
   input ``-1``, ``0``, or ``1``.

#### (c) Embedding Example

Our dataset, `features.json`, contains the input `U` and output `v`. Each record
in `U` is a vector with two features. We consider three embedding strategies for
the input:

1. ``ϕ₁(u) = (1, u₁, u₂)``
2. ``ϕ₂(u) = (1, u₁, u₂, u₁u₂)``
3. ``ϕ₃(u) = (1, u₁, u₂, u₁u₂, log(u₁), log(u₂))``

And the embedding ``ψ(v) = log(v)`` for the output.

```julia
ϕ1(u) = hcat(ones(size(u, 1)), u)
ϕ2(u) = hcat(ones(size(u, 1)), u, prod(u, dims=2))
ϕ3(u) = hcat(ones(size(u, 1)), u, prod(u, dims=2), log.(u))
ψ(v) = log.(v)
;
```

Let's put our embeddings to work:

```julia
# Load up dataset
data = readclassjson("./data/features.json")
U, v = data["U"], data["v"]

# Split into train and test sets
train_set, test_set = split_dataset(U, v)
train_u, train_v = train_set.input, train_set.output
test_u, test_v = test_set.input, test_set.output

# Regression with embeddings
embeddings = [ϕ1, ϕ2, ϕ3]
for embed in embeddings
    train_x = embed(train_u)
    train_y = ψ(train_v)
    θ, _, _ = find_theta(train_x, train_y)

    test_x = embed(test_u)
    test_y = ψ(test_v)
    test_ŷ = test_x * θ
    println("RMSE for embedding $(nameof(embed)): ", RMSE(test_ŷ, test_y))
end
```

The RMSE is not the best, but it seems the third embedding strategy is our best
bet.

#### (d) Unembedding

In the last section, we choose to embed the raw output, but we are not so much
interested in the embedded output, so let's define an unembedding function to
convert our predictions back to raw output format.

```julia
unembed(y) = exp.(y)

train_x = ϕ3(train_u)
train_y = ψ(train_v)
θ, _, _ = find_theta(train_x, train_y)

test_x = ϕ3(test_u)
test_y = ψ(test_v)
test_ŷ = test_x * θ
test_v̂ = unembed(test_ŷ)
@show RMSE(test_v̂, test_v);
```

## 3. Moore's Law

We examine Gordon Moore's prediction that the number of transistors doubles
every two years. The model, provided in Boyd's and Vandenberghe's book
*Introduction to Applied Linear Algebra*, is as follows:

```math
log_{10}N ≈ θ_1 + θ_{2}(t - 1970).
```

```julia
year = [1971, 1972, 1974, 1978, 1982, 1985, 1989, 1993, 1997, 1999, 2000, 2002, 2003]
transistors = [2250, 2500, 5000, 29_000, 120_000, 275_000, 1_180_000, 3_100_000,
               7_500_000, 24_000_000, 42_000_000, 220_000_000, 410_000_000]
;
```

#### (a) Least Squares Regression

```julia
ϕ(year) = hcat(ones(length(year)), year .- 1970)
ψ(transistors) = log10.(transistors)

# Regression
x = ϕ(year)
y = ψ(transistors)
θ, _, error = find_theta(x, y)
@show θ
@show error

# Plot the model
scatter(year, transistors, yscale=:log10, label="")
xrange = 1970:0.5:2005
ŷ = 10 .^ (ϕ(xrange) * θ)
plot!(xrange, ŷ, yscale=:log10, label="",
      xlabel="Year", ylabel="Number of transistors", title="Moore's Law")
```

#### (b) Number of Transistors in 2015

```julia
model_2015 = 10^(θ[1] + θ[2] * (2015 - 1970))
```

Our model is not so accurate to predict the ``4 × 10^9`` transistors IBM Z13
microprocessor has in 2015. This suggests that an exponent function is not the
best model in the long run: we are approaching the physical limit of a size of
an atom for transistors.

#### (c) Moore's Prediction

```julia
growth_rate = 10 ^ θ[2] # per year
growth_rate^2
```

Our model is not so much different from Moore's Law! We also predict that the
number of transistors would roughly double every two years.
